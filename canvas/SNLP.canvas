{
	"nodes":[
		{"id":"aed1321b156cff68","type":"file","file":"ETC/WhatsApp Image 2024-03-07 at 17.10.10_79b70d28.jpg","x":280,"y":160,"width":300,"height":400},
		{"id":"f59a57452af582cc","type":"file","file":"ETC/WhatsApp Image 2024-03-07 at 17.10.11_03b13e55.jpg","x":620,"y":240,"width":267,"height":400},
		{"id":"9f6b1cf29bedfa12","type":"file","file":"ETC/WhatsApp Image 2024-03-07 at 17.10.09_81e11286.jpg","x":-78,"y":20,"width":304,"height":540},
		{"id":"40f3bd698f4b9996","type":"file","file":"Excalidraw/Drawing 2024-03-18 02.23.26.excalidraw.md","x":-75,"y":640,"width":829,"height":540},
		{"id":"5105dc88bd2af747","type":"text","text":"## Unit 5","x":-40,"y":1400,"width":415,"height":200,"color":"5"},
		{"id":"55593b826d731043","type":"text","text":"trouble, troubled, troubles -> troubl (SHORTEN TO SOME ROOT WORD)","x":1956,"y":1610,"width":364,"height":110},
		{"id":"8bd9b2b76a4d578f","type":"text","text":"Same sounding but different spelling -> reduce to some same token for speech processing programs","x":1956,"y":1740,"width":250,"height":60},
		{"id":"50b46772d5957bd3","type":"text","text":"Useful in IR - Information retrieval systems","x":1956,"y":1820,"width":250,"height":60},
		{"id":"de9ceecf4c40156a","type":"text","text":"Accents, UK/US, etc","x":1956,"y":1900,"width":250,"height":60},
		{"id":"7ab9f504629f5513","type":"text","text":"Stemming","x":1600,"y":1630,"width":250,"height":60},
		{"id":"3c7724753f453b40","type":"text","text":"Sentence Division","x":1600,"y":1710,"width":250,"height":60},
		{"id":"7b256f62eebc7c27","type":"text","text":"Phonetic Normalization\n","x":1600,"y":1790,"width":250,"height":60},
		{"id":"dbc885ec45e3d9ec","type":"text","text":"Spelling Correction","x":1600,"y":1870,"width":250,"height":60,"color":"1"},
		{"id":"5a8ee337091ca947","type":"text","text":"Others","x":1600,"y":1950,"width":250,"height":60},
		{"id":"f70840f717ad8062","type":"text","text":"Tokenization","x":1600,"y":1470,"width":250,"height":60},
		{"id":"f051cd1ef9a5cbad","type":"text","text":"Lemmitization","x":1600,"y":1550,"width":250,"height":60},
		{"id":"dcd938f2ed7cf28d","type":"text","text":"sang sung sings -> sing","x":1956,"y":1530,"width":250,"height":60},
		{"id":"a4e6ed077aead144","type":"text","text":"Tasks","x":952,"y":1643,"width":250,"height":60},
		{"id":"c4dbfc6723e1af1a","type":"text","text":"Normalization","x":519,"y":1550,"width":250,"height":60},
		{"id":"99507b5af0e32c36","type":"text","text":"Applications:\n- IR - Query Expansion, Regular normalization ('U.S.A.' and 'USA')\n- Conversational AI - ASR, TTS, (\"five p m\", ASR should interpret this as \"5:00PM\". This is called inverse text normalization. Dr. could be drive or Doctor, context matters)\n- Egs of NLP tasks that benefit from normalization: Machine transaltion, opinion mining, spell checking, sentiment analysis, dependency parsing, named entity analysis","x":952,"y":2200,"width":585,"height":252},
		{"id":"fba5b1a2fa4d90d4","type":"text","text":"Approaches","x":952,"y":2680,"width":250,"height":60},
		{"id":"c7e7e336c5fc3c68","type":"text","text":"Mapping\nRule based\nDistance Based\nSpelling correction\n\nASR\nMachine Learning\nNeural Networks\n","x":1288,"y":2621,"width":492,"height":259},
		{"id":"0e30a2051e5cffa4","type":"text","text":"Problems","x":2338,"y":1331,"width":250,"height":60},
		{"id":"636e42312bd0df4c","type":"text","text":"Contractions, Social Media text, hyphens.\n\nJapanese,Chinese, no whitespaces to separate words, greedy algos find longest dict word instead\n\nFrench \"L'ensemble\" -> L,L' or Le?\n\nGerman noun compounds aren't segmented.","x":2721,"y":1255,"width":479,"height":305},
		{"id":"8518c1f61fc0a709","type":"text","text":"Approaches","x":2328,"y":1070,"width":250,"height":60},
		{"id":"0d877222cf2b0d2c","type":"text","text":"Byte Pair encoding\nWordPiece\nSentencePiece","x":2721,"y":972,"width":285,"height":128},
		{"id":"0b3e8340b19194d9","type":"text","text":"Problems","x":952,"y":3080,"width":250,"height":60},
		{"id":"d093bd696a95c767","type":"text","text":"Social Media Text","x":1306,"y":3062,"width":250,"height":60},
		{"id":"1bc920f8c3733cfa","type":"text","text":"Abbreviations\nMisspellings\nOmmited Punctuations\nSlang\nWordplay\nEmoticons\nDisguised Vulgarities\n","x":1709,"y":2998,"width":291,"height":182},
		{"id":"483b82f90a235721","type":"text","text":"Social Media Pipeline\n![[Pasted image 20240318162358.png]]","x":1709,"y":3280,"width":486,"height":639},
		{"id":"dbbc6e9076213d69","type":"file","file":"WhatsApp Image 2024-03-18 at 14.34.54_a22c2a7e.jpg","x":920,"y":240,"width":360,"height":480},
		{"id":"d3b58eff7bb86ed2","type":"file","file":"SNLP - CIE-2 (MSA).pdf","x":1380,"y":660,"width":470,"height":560},
		{"id":"31ea1acaea33fd31","type":"file","file":"Snlp Cie-1 Imp(MSA).pdf","subpath":"#page=1","x":1907,"y":2065,"width":593,"height":815},
		{"id":"f34afda20bf38976","type":"file","file":"canvas/SNLP Answers Internal 2.canvas","x":865,"y":1130,"width":380,"height":400},
		{"id":"7262758f3063f800","type":"text","text":"## Unit 1","x":2460,"y":-460,"width":291,"height":169},
		{"id":"c805c6ae79c4315a","type":"text","text":"## Unit 2","x":2460,"y":-160,"width":291,"height":169},
		{"id":"da5818fd1b703597","type":"text","text":"Origins and Challenges off NLP","x":3087,"y":-703,"width":250,"height":58},
		{"id":"0563e5f16341bed3","type":"file","file":"ETC/Drawing 2024-03-18 22.38.05.excalidraw.md","x":1760,"y":-620,"width":610,"height":768},
		{"id":"2346d1071f1318d2","type":"file","file":"Origin and challenges of NLP.md","x":3680,"y":-1580,"width":825,"height":1260},
		{"id":"8d716ac5af2e1e3f","x":3680,"y":-236,"width":680,"height":896,"type":"text","text":"Regular expressions, often abbreviated as regex or regexp, are sequences of characters used to define search patterns. They provide a concise and flexible means of matching strings of text based on specific patterns of characters.\n\nHere's a breakdown of the main components and functionalities of regular expressions:\n\n1. *Literals:* Regular expressions can include literal characters, which match themselves exactly. For example, the regular expression \"cat\" matches the string \"cat\" in a text.\n\n2. *Metacharacters:* Metacharacters are special characters in regular expressions that don't match themselves literally but instead have special meanings. Examples include:\n   - . (dot): Matches any single character except newline.\n   - *, +, ?: Quantifiers that specify the number of occurrences of the preceding character or group.\n   - []: Character classes that match any one of the characters within the brackets.\n   - ^, $: Anchors that match the start and end of a line, respectively.\n   - |: Alternation operator that matches either the expression before or after it.\n   - \\\\: Escape character used to escape metacharacters to match them literally.\n\n3. *Grouping and Capturing:* Parentheses ( ) are used for grouping expressions together and capturing matched substrings for later use or reference.\n\n4. *Modifiers:* Modifiers are optional flags that can be added to the end of a regular expression to specify how the pattern should be matched. Common modifiers include:\n   - i: Case-insensitive matching.\n   - g: Global matching (matches all occurrences, not just the first one).\n   - m: Multiline mode (treats beginning and end anchors as the start and end of lines).\n\nRegular expressions are widely used in text processing tasks such as search and replace operations, text validation, pattern extraction, and more. They are supported in various programming languages and tools, such as Python, JavaScript, Perl, and Unix utilities like grep and sed. Learning regular expressions can greatly enhance your ability to manipulate and process text effectively."},
		{"id":"45d0bb0d069cd73f","x":3087,"y":-190,"width":250,"height":60,"type":"text","text":"Regular Expressions"},
		{"id":"d2e873c1b1150e38","x":3568,"y":817,"width":452,"height":253,"type":"text","text":"Study of.... from Morphemes\n\nMorphemes are\nEg:\n\nTwo classes of morphemes\n\nConcatenative morphology:\n2 types."},
		{"id":"2af55771baff8592","x":3075,"y":817,"width":250,"height":60,"type":"text","text":"Morphology"},
		{"id":"c6ad6a4fbebfdf6e","x":3589,"y":1211,"width":250,"height":60,"type":"text","text":"FSTs"},
		{"id":"4df1e03056ef2163","x":4120,"y":1211,"width":893,"height":915,"type":"text","text":"Finite State Transducers (FSTs) are computational models used in various fields, including natural language processing, speech recognition, and computational linguistics. FSTs extend the concept of Finite State Machines (FSMs) to represent transformations between sequences of symbols.\n\nHere are key aspects of Finite State Transducers:\n\n1. *Finite State Machines (FSMs):*\n   - FSTs are an extension of FSMs, which are abstract computational models consisting of states, transitions, and input symbols.\n   - In an FSM, transitions between states are determined by input symbols, leading to the recognition or generation of sequences of symbols.\n   - FSMs are used to model systems with finite memory or computational capabilities.\n\n2. *Transducers:*\n   - In the context of FSTs, transducers represent machines that perform transformations on input sequences to produce output sequences.\n   - Transducers have states and transitions like FSMs but also associate each transition with an output symbol or sequence.\n\n3. *Input and Output Alphabets:*\n   - FSTs operate over input and output alphabets, which consist of symbols or characters.\n   - Input symbols represent the symbols to be transformed or recognized, while output symbols represent the result of the transformation.\n\n4. *Applications:*\n   - FSTs are used in various applications, including:\n     - Morphological analysis and generation in natural language processing.\n     - Spelling correction and autocomplete in text processing.\n     - Speech recognition and synthesis.\n     - Finite-state phonology in computational linguistics.\n   \n5. *Properties:*\n   - FSTs are capable of representing regular relations as well as transformations between sequences of symbols.\n   - They are computationally efficient and can be implemented using finite memory and processing resources.\n\nIn summary, Finite State Transducers are computational models that extend the concept of Finite State Machines to represent transformations between sequences of symbols, making them valuable tools in various fields where sequential data processing is required."},
		{"id":"6479f49db31d9122","x":5304,"y":1485,"width":596,"height":335,"type":"text","text":"FSTs are extensions of FSms that ...\n\nFSMs:...\n\nTransducers:...\n\nInput and output alphabets\n\nApplications\n\nProperties:"},
		{"id":"065f0515c317c02e","x":-200,"y":2279,"width":926,"height":863,"type":"text","text":"*Diphone Waveform Synthesis*\n\nDiphone waveform synthesis involves using units called diphones, which represent segments of speech extending from roughly the middle of one phoneme to the middle of the following phoneme. The process of diphone concatenative synthesis can be outlined as follows:\n\n*Training:*\n\n1. *Recording:* Record a single speaker saying an example of each diphone.\n2. *Database Creation:* Cut out each diphone from the recorded speech and store all diphones in a dedicated database.\n\n*Synthesis:*\n\n1. *Selection:* From the diphone database, select a sequence of diphones that corresponds to the desired sequence of phonemes.\n2. *Concatenation:* Concatenate the selected diphones, applying slight signal processing at the boundaries to ensure smooth transitions.\n3. *Prosody Adjustment:* Use signal processing techniques to modify the prosody (such as fundamental frequency and duration) of the diphone sequence to match the desired prosodic characteristics.\n\n*Building a Diphone Database*\n\nCreating a diphone database involves several steps:\n\n1. *Diphone Inventory:* Determine the set of diphones needed for the target language or speech synthesis system.\n2. *Speaker Recruitment:* Enlist a suitable speaker to record the diphones.\n3. *Text Creation:* Prepare a text containing examples of each diphone for the speaker to read aloud.\n4. *Recording:* Record the speaker reading the provided text, capturing examples of each diphone.\n5. *Segmentation and Labeling:* Segment the recorded speech into individual diphones, labeling them accordingly. Additionally, mark pitch contours to capture prosodic information.\n6. *Diphone Extraction:* Extract and store the segmented diphones for use in the synthesis process."},
		{"id":"f7cc61db3bcb4cfe","x":2492,"y":214,"width":250,"height":60,"type":"text","text":"N grams"},
		{"id":"5fbcfead0eead686","x":2300,"y":532,"width":250,"height":60,"type":"text","text":"Stochastic POS Taggers"},
		{"id":"c65cff89b0d6fd97","x":3040,"y":-14,"width":379,"height":288,"type":"text","text":"Here, a probability dist for a ....\n\nn=4 eg\n\nuni,bi,tri gram\n\nthe cow jumps over the grass Example with various grams\n\nHow many n grams can we have in a sentence"},
		{"id":"97f78538b9ca7cd0","x":2657,"y":460,"width":349,"height":260,"type":"text","text":"any model that includes...\n\n\nsimplest technique is...\n\n2nd technique\n\npipeline example"},
		{"id":"51a97d00a4a50e05","x":2053,"y":647,"width":297,"height":233,"type":"text","text":"POS tagging:\n\nWhat it is\n\nPurpose\n\nPerformed using: two things with details\n\n"},
		{"id":"ed3d8af661d82193","x":2401,"y":731,"width":298,"height":305,"type":"text","text":"Problems:\n\nAmbiguity\n\nDomain Problems\n\nLanguage Ambiguity \n\nContext\n\nTagging errors\n\nOut of vocabulary words"},
		{"id":"00b9949e5c4e0e9d","x":2903,"y":-1380,"width":618,"height":622,"type":"text","text":"Language Modeling (LM)\n\nLanguage models (LMs) serve to estimate the likelihood of different words or phrases relative to others, enhancing various NLP applications. Most LM-based models utilize probabilistic inferences to gauge the probability of upcoming words the user might input, or to assess the likelihood of specific words or phrases within existing data.\n\nThe probability \\( P(w) \\) of a sequence of words \\( w_1, w_2, w_3, \\ldots, w_n \\) occurring is calculated using the equation:\n\n\\[ P(w) = P(w_1, w_2, w_3, \\ldots, w_n) = \\prod P(w_i | w_1, w_2, w_3, \\ldots, w_{i-1}) \\]\n\nwhere \\( n \\) represents the total occurrences in the given data and \\( i \\) signifies the instance for which we seek occurrence.\n\n1. Grammar-Based Language Modeling\n\nThis model, as the name implies, relies on grammatical inferences provided to predict sentences based on probabilistic inferences derived from past occurrences. The language to interpret a particular event is deduced from historical data. Let's illustrate this model with an example:\n\nExample: Sentence: \"This glass is transparent.\" \\( P(\"This glass is transparent.\") = P(w_i) \\)\n\nTo construct or predict the above sentence, we consider all possible combinations based on the given rule:\n\n1. \\( P(This ) = P(w_1) \\)\n2. \\( P(glass | This ) = P(w_2) \\)\n3. \\( P( is | This glass ) = P(w_3) \\)\n4. \\( P(transparent | This glass is ) = P(w_4) \\)\n\nThe probability of the sentence is then calculated using the formula: \\( P(w) = \\prod P(w_i | w_1, w_2, w_3, \\ldots, w_{i-1}) \\), yielding the likelihood of a grammatically correct statement, which can be presented to the user. This forms the basis for sentence or phrase completion models in NLP.\n\n2. Statistical-Based Language Modeling\n\nThis model, as its name suggests, relies on statistical inferences drawn from input data. It heavily depends on correctly formatted and appropriate input data. Probability distributions are established over required word sequences, which are then applied to textual data for inferences based on assigned values."}
	],
	"edges":[
		{"id":"07567e6fe3bbeb4d","fromNode":"5105dc88bd2af747","fromSide":"right","toNode":"c4dbfc6723e1af1a","toSide":"left"},
		{"id":"37137d44cb3371bb","fromNode":"a4e6ed077aead144","fromSide":"right","toNode":"f70840f717ad8062","toSide":"left"},
		{"id":"78ffaeb717767653","fromNode":"a4e6ed077aead144","fromSide":"right","toNode":"f051cd1ef9a5cbad","toSide":"left"},
		{"id":"3bab774430867eb3","fromNode":"a4e6ed077aead144","fromSide":"right","toNode":"7ab9f504629f5513","toSide":"left"},
		{"id":"cbe6a803145a59af","fromNode":"a4e6ed077aead144","fromSide":"right","toNode":"3c7724753f453b40","toSide":"left"},
		{"id":"6132c78582aefda5","fromNode":"a4e6ed077aead144","fromSide":"right","toNode":"7b256f62eebc7c27","toSide":"left"},
		{"id":"8828beca5c3ea44f","fromNode":"a4e6ed077aead144","fromSide":"right","toNode":"5a8ee337091ca947","toSide":"left"},
		{"id":"b632213a1f068f52","fromNode":"a4e6ed077aead144","fromSide":"right","toNode":"dbc885ec45e3d9ec","toSide":"left"},
		{"id":"70c67d2f98926b62","fromNode":"f051cd1ef9a5cbad","fromSide":"right","toNode":"dcd938f2ed7cf28d","toSide":"left"},
		{"id":"9304f9c49040fe49","fromNode":"7ab9f504629f5513","fromSide":"right","toNode":"55593b826d731043","toSide":"left"},
		{"id":"fc57fe50bc130e8b","fromNode":"7b256f62eebc7c27","fromSide":"right","toNode":"8bd9b2b76a4d578f","toSide":"left"},
		{"id":"01f3abb6ff29f4e4","fromNode":"dbc885ec45e3d9ec","fromSide":"right","toNode":"50b46772d5957bd3","toSide":"left"},
		{"id":"c4e2b66c009f420b","fromNode":"5a8ee337091ca947","fromSide":"right","toNode":"de9ceecf4c40156a","toSide":"left"},
		{"id":"b452b743895f75e8","fromNode":"c4dbfc6723e1af1a","fromSide":"right","toNode":"a4e6ed077aead144","toSide":"left"},
		{"id":"2279ccbf932632f4","fromNode":"c4dbfc6723e1af1a","fromSide":"right","toNode":"99507b5af0e32c36","toSide":"left"},
		{"id":"190be71fad6be081","fromNode":"c4dbfc6723e1af1a","fromSide":"right","toNode":"fba5b1a2fa4d90d4","toSide":"left"},
		{"id":"26c4c3769925d2fe","fromNode":"fba5b1a2fa4d90d4","fromSide":"right","toNode":"c7e7e336c5fc3c68","toSide":"left"},
		{"id":"3a25fcb9e50f0220","fromNode":"f70840f717ad8062","fromSide":"right","toNode":"0e30a2051e5cffa4","toSide":"left"},
		{"id":"5cd5db9a3d240066","fromNode":"0e30a2051e5cffa4","fromSide":"right","toNode":"636e42312bd0df4c","toSide":"left"},
		{"id":"e2b0829987384f8a","fromNode":"f70840f717ad8062","fromSide":"right","toNode":"8518c1f61fc0a709","toSide":"left"},
		{"id":"8fe679d6689a49c5","fromNode":"8518c1f61fc0a709","fromSide":"right","toNode":"0d877222cf2b0d2c","toSide":"left"},
		{"id":"c867a3a28e9e940f","fromNode":"c4dbfc6723e1af1a","fromSide":"right","toNode":"0b3e8340b19194d9","toSide":"left"},
		{"id":"8028ba470f89df0c","fromNode":"0b3e8340b19194d9","fromSide":"right","toNode":"d093bd696a95c767","toSide":"left"},
		{"id":"9e58e3cdfdae4ef6","fromNode":"d093bd696a95c767","fromSide":"right","toNode":"1bc920f8c3733cfa","toSide":"left"},
		{"id":"e3ddedaafec4cc4b","fromNode":"d093bd696a95c767","fromSide":"right","toNode":"483b82f90a235721","toSide":"left"},
		{"id":"d10335b6afa2e5ed","fromNode":"5105dc88bd2af747","fromSide":"right","toNode":"f34afda20bf38976","toSide":"left"},
		{"id":"2895f47825d6bba8","fromNode":"f34afda20bf38976","fromSide":"right","toNode":"d3b58eff7bb86ed2","toSide":"left"},
		{"id":"f1c655208d9aa7f6","fromNode":"7262758f3063f800","fromSide":"right","toNode":"da5818fd1b703597","toSide":"left"},
		{"id":"094d885525364367","fromNode":"da5818fd1b703597","fromSide":"right","toNode":"2346d1071f1318d2","toSide":"left"},
		{"id":"43d9aea079c1f182","fromNode":"0563e5f16341bed3","fromSide":"right","toNode":"7262758f3063f800","toSide":"left"},
		{"id":"d741badd0b13ee9e","fromNode":"0563e5f16341bed3","fromSide":"right","toNode":"c805c6ae79c4315a","toSide":"left"},
		{"id":"cb217072145c2195","fromNode":"aed1321b156cff68","fromSide":"right","toNode":"0563e5f16341bed3","toSide":"left","color":"4"},
		{"id":"0f5da23f8ae4d21b","fromNode":"7262758f3063f800","fromSide":"right","toNode":"45d0bb0d069cd73f","toSide":"left"},
		{"id":"f1e0fbaefbaf880f","fromNode":"45d0bb0d069cd73f","fromSide":"right","toNode":"8d716ac5af2e1e3f","toSide":"left"},
		{"id":"7ca2bd56ecf03c25","fromNode":"7262758f3063f800","fromSide":"right","toNode":"2af55771baff8592","toSide":"left"},
		{"id":"6f94f32a61e85c98","fromNode":"2af55771baff8592","fromSide":"right","toNode":"d2e873c1b1150e38","toSide":"left"},
		{"id":"748f1178e96d6c08","fromNode":"7262758f3063f800","fromSide":"right","toNode":"c6ad6a4fbebfdf6e","toSide":"left"},
		{"id":"3cc729656391e470","fromNode":"c6ad6a4fbebfdf6e","fromSide":"right","toNode":"4df1e03056ef2163","toSide":"left"},
		{"id":"314bfa27709c89d1","fromNode":"4df1e03056ef2163","fromSide":"right","toNode":"6479f49db31d9122","toSide":"left"},
		{"id":"4e1765ca538ba07a","fromNode":"5105dc88bd2af747","fromSide":"right","toNode":"065f0515c317c02e","toSide":"left"},
		{"id":"057d91cc5220fc07","fromNode":"c805c6ae79c4315a","fromSide":"right","toNode":"f7cc61db3bcb4cfe","toSide":"left"},
		{"id":"29171ee69560a7f4","fromNode":"c805c6ae79c4315a","fromSide":"right","toNode":"5fbcfead0eead686","toSide":"left"},
		{"id":"1a364f96497f56c7","fromNode":"f7cc61db3bcb4cfe","fromSide":"right","toNode":"c65cff89b0d6fd97","toSide":"left"},
		{"id":"9afbf0a940de15bf","fromNode":"5fbcfead0eead686","fromSide":"right","toNode":"97f78538b9ca7cd0","toSide":"left"},
		{"id":"8dfde4221d5c11da","fromNode":"c805c6ae79c4315a","fromSide":"right","toNode":"51a97d00a4a50e05","toSide":"left"},
		{"id":"a15e29fb515f6b3b","fromNode":"51a97d00a4a50e05","fromSide":"right","toNode":"ed3d8af661d82193","toSide":"left"},
		{"id":"fba2e8961790b3d8","fromNode":"7262758f3063f800","fromSide":"right","toNode":"00b9949e5c4e0e9d","toSide":"left"}
	]
}